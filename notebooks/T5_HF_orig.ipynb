{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook adapted from:  \n",
    "https://medium.com/askdata/train-t5-for-text-summarization-a1926f52d281  \n",
    "https://colab.research.google.com/drive/14_A2kM8sOVpzwHn-0pMbfnD2htzI2Nte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false  --no-raise-error\n",
    "# comment out this first line to install the correct version of transformers and datasets\n",
    "# you may want to create a conda env for this, since this a dev branch\n",
    "\n",
    "!git clone https://github.com/huggingface/transformers\n",
    "%mv transformers ../software/transformers\n",
    "%cd ../software/transformers\n",
    "!git checkout t5-fp16-no-nans\n",
    "!pip install . --upgrade\n",
    "!pip install datasets\n",
    "%cd ../../notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from tqdm.notebook import tqdm\n",
    "from torch import nn\n",
    "\n",
    "SEED = 2557\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6.0.dev0\n",
      "1.8.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Weights & Biases for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbryanli\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_LOG_MODEL=true\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "%env WANDB_LOG_MODEL=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_TRAIN = True\n",
    "DO_EVAL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/nlpgridio3/data/bryanli/projects/stories/glucose\n"
     ]
    }
   ],
   "source": [
    "%cd ../glucose/\n",
    "\n",
    "GLUCOSE_DIR = os.getcwd()\n",
    "TRAIN_PATH = os.path.join(GLUCOSE_DIR, 'data_final/GLUCOSE_training_data_final.csv')\n",
    "TEST_PATH = os.path.join(GLUCOSE_DIR, 'data_final/nov27_key_final_copy.csv')\n",
    "\n",
    "from scripts import format_data\n",
    "from scripts.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = 't5-large'\n",
    "# model_size = 't5-base'\n",
    "\n",
    "suffix = '_' + model_size.split('-')[-1]\n",
    "\n",
    "exp_num = '0'\n",
    "EXP_NAME = f'exp{exp_num}{suffix}'\n",
    "OUTPUT_DIR = f'{GLUCOSE_DIR}/outputs/{EXP_NAME}'\n",
    "MODEL_DIR =  f'{OUTPUT_DIR}/model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting 1: Generation\n",
    "Here, we frame the task as a generation problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data and format it for our experiments. The options for `exp_num` are:  \n",
    "'0' : same as the original task\n",
    "'1' : input = dim + precontext, output = target sentence  \n",
    "'2a': input = dim + precontext, output = original output (with generalized and contextualized)  \n",
    "'2b': input = dim + precontext + \\<mask_sent\\> + postcontext, output = original output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train: 273952\n",
      "size of validation: 30147\n"
     ]
    }
   ],
   "source": [
    "df_train, df_val, ids_val = format_data.format_data(TRAIN_PATH, exp_num, split_val=True, seed=SEED)\n",
    "\n",
    "df_test, _, _ = format_data.format_data(TEST_PATH, exp_num, split_val=False, seed=SEED, is_test=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "with open(OUTPUT_DIR + '/ids_val.txt', 'w') as f:\n",
    "    f.writelines([f'{idx}\\n' for idx in ids_val])\n",
    "        \n",
    "print(f\"size of train: {len(df_train)}\")\n",
    "print(f\"size of validation: {len(df_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "experiment_id              a2bb14bd-9094-4dcc-93a5-bb93b1c308a5__4\n",
       "story_id                      a2bb14bd-9094-4dcc-93a5-bb93b1c308a5\n",
       "input            #1: my daughter had to do a science project. s...\n",
       "specific         My daughter has  a  science project >Causes/En...\n",
       "general          Someone_B (who is Someone_A's child) has a sci...\n",
       "output           My daughter has a science project >Causes/Enab...\n",
       "Name: 10, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: #1: It was bedtime at our house. Two of the three kids hit the pillow and fall asleep. The third is a trouble maker. For two hours he continues to get out of bed and want to play. *Finally he becomes tired and falls asleep.*\n",
      "output: The third kid continues to get out of bed and wants to play >Causes/Enables> The kid finally becomes tired and falls asleep ** Someone_A doesn't want to go to sleep >Causes/Enables> Someone_A finally falls asleep\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#1: It was bedtime at our house. Two of the th...</td>\n",
       "      <td>The third kid continues to get out of bed and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#3: It was bedtime at our house. Two of the th...</td>\n",
       "      <td>The third kid is in bed &gt;Enables&gt; The kid fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#1: Sara was invited to the football game. She...</td>\n",
       "      <td>Sara goes to a football game &gt;Causes/Enables&gt; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#3: Sara was invited to the football game. She...</td>\n",
       "      <td>Sara is at a football game &gt;Enables&gt; Her team ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#5: Sara was invited to the football game. She...</td>\n",
       "      <td>The game is over &gt;Enables&gt; Sara's team wins **...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304094</th>\n",
       "      <td>#3: Julie was on the couch when she saw her mo...</td>\n",
       "      <td>Julie and her mom are at home &gt;Enables&gt; Julie'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304095</th>\n",
       "      <td>#4: Julie was on the couch when she saw her mo...</td>\n",
       "      <td>Julie's mom possess(es) sandwich ingredients &gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304096</th>\n",
       "      <td>#6: Julie was on the couch when she saw her mo...</td>\n",
       "      <td>Julie's mom agrees to prepare a sandwich for J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304097</th>\n",
       "      <td>#7: Julie was on the couch when she saw her mo...</td>\n",
       "      <td>Julie's mom agrees to prepare a sandwich for J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304098</th>\n",
       "      <td>#9: Julie was on the couch when she saw her mo...</td>\n",
       "      <td>Julie's mom agrees to prepare a sandwich for J...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>273952 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    input  \\\n",
       "0       #1: It was bedtime at our house. Two of the th...   \n",
       "1       #3: It was bedtime at our house. Two of the th...   \n",
       "2       #1: Sara was invited to the football game. She...   \n",
       "3       #3: Sara was invited to the football game. She...   \n",
       "4       #5: Sara was invited to the football game. She...   \n",
       "...                                                   ...   \n",
       "304094  #3: Julie was on the couch when she saw her mo...   \n",
       "304095  #4: Julie was on the couch when she saw her mo...   \n",
       "304096  #6: Julie was on the couch when she saw her mo...   \n",
       "304097  #7: Julie was on the couch when she saw her mo...   \n",
       "304098  #9: Julie was on the couch when she saw her mo...   \n",
       "\n",
       "                                                   output  \n",
       "0       The third kid continues to get out of bed and ...  \n",
       "1       The third kid is in bed >Enables> The kid fina...  \n",
       "2       Sara goes to a football game >Causes/Enables> ...  \n",
       "3       Sara is at a football game >Enables> Her team ...  \n",
       "4       The game is over >Enables> Sara's team wins **...  \n",
       "...                                                   ...  \n",
       "304094  Julie and her mom are at home >Enables> Julie'...  \n",
       "304095  Julie's mom possess(es) sandwich ingredients >...  \n",
       "304096  Julie's mom agrees to prepare a sandwich for J...  \n",
       "304097  Julie's mom agrees to prepare a sandwich for J...  \n",
       "304098  Julie's mom agrees to prepare a sandwich for J...  \n",
       "\n",
       "[273952 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = df_train.iloc[0]\n",
    "print(f'input: {ex[\"input\"]}')\n",
    "print(f'output: {ex[\"output\"]}')\n",
    "df_train[['input', 'output']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up wand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.26<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">exp0_large</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/bryanli/glucose_final\" target=\"_blank\">https://wandb.ai/bryanli/glucose_final</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/bryanli/glucose_final/runs/3jdejpon\" target=\"_blank\">https://wandb.ai/bryanli/glucose_final/runs/3jdejpon</a><br/>\n",
       "                Run data is saved locally in <code>/mnt/nlpgridio3/data/bryanli/projects/stories/glucose/wandb/run-20210527_012939-3jdejpon</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if DO_TRAIN:\n",
    "    wandb.init(project=\"glucose_final\", name=EXP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_size)\n",
    "\n",
    "if exp_num == '2b':\n",
    "    special_tokens_dict = {'additional_special_tokens': ['<mask_sent>']}\n",
    "    add_toks = tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = datasets.Dataset.from_pandas(df_train)\n",
    "ds_val = datasets.Dataset.from_pandas(df_val)\n",
    "ds_test = datasets.Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_source, max_target = get_src_tgt_len(df_train['input'], df_train['output'], tokenizer)\n",
    "print(max_source, max_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_ENCODE = 512\n",
    "\n",
    "kwargs = dict(max_source=max_source, max_target=max_target, tokenizer=tokenizer)\n",
    "ds_train = ds_train.map(encode, batched=True, batch_size=BATCH_SIZE_ENCODE, fn_kwargs=kwargs)\n",
    "ds_val = ds_val.map(encode, batched=True, batch_size=BATCH_SIZE_ENCODE, fn_kwargs=kwargs)\n",
    "ds_test = ds_test.map(encode, batched=True, batch_size=BATCH_SIZE_ENCODE, fn_kwargs=kwargs)\n",
    "\n",
    "# ds_train.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "# ds_val.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "# ds_test.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS_TO_FORMAT = ['input_ids', 'labels', 'attention_mask']\n",
    "ds_train.set_format(type='torch', columns=COLS_TO_FORMAT)\n",
    "ds_val.set_format(type='torch', columns=COLS_TO_FORMAT)\n",
    "ds_test.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify proper encoding\n",
    "print(tokenizer.decode(ds_val[0]['input_ids']))\n",
    "print()\n",
    "print(tokenizer.decode(ds_val[0]['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train.save_to_disk(f'{OUTPUT_DIR}/ds_train')\n",
    "ds_val.save_to_disk(f'{OUTPUT_DIR}/ds_val')\n",
    "ds_test.save_to_disk(f'{OUTPUT_DIR}/ds_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_TRAIN:\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_size, cache_dir='/nlp/data/bryanli/.cache')\n",
    "    if exp_num == '2b':\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    ds_train_shuffled = ds_train.shuffle(seed=SEED)\n",
    "    ds_val_shuffled = ds_val.shuffle(seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using `t5-large`, you will need to be on nlpgpu03, which has 24 GB vRAM per GPU. The `t5-large` model needs ~10 GB alone + the size of batches.\n",
    "\n",
    "With either `t5-large` or `t5-base`, we find that we need to train for far less epochs than described in the paper (which says \"500K steps\" without clearly specifying what that means) for validation loss to stop decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if DO_TRAIN:\n",
    "    trainer = None\n",
    "    # taken from GLUCOSE and T5 paper when possible\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=12,\n",
    "        # prediction_loss_only=True, # If I need co compute only loss and not other metrics, setting this to true will use less RAM\n",
    "        evaluation_strategy='steps', # Run evaluation every eval_steps\n",
    "        save_steps=1000, # How often to save a checkpoint\n",
    "        logging_steps=1000, # How often to log loss to wandb\n",
    "        save_total_limit=10, # Number of maximum checkpoints to save\n",
    "        remove_unused_columns=True, # Removes useless columns from the dataset\n",
    "        run_name=EXP_NAME, # Wandb run name\n",
    "        load_best_model_at_end=True, # Whether to load the best model found at each evaluation.\n",
    "        metric_for_best_model=\"eval_loss\", # Use loss to evaluate best model.\n",
    "        greater_is_better=False, # Best model is the one with the lowest loss, not highest.\n",
    "        eval_accumulation_steps=10,\n",
    "#         fp16=True # doesn't work for t5-large yet\n",
    "    )\n",
    "    optimizer = transformers.Adafactor(model.parameters(), lr=0.0001,\n",
    "                                   relative_step=False, warmup_init=False, scale_parameter=False,\n",
    "                                   decay_rate=0.0, clip_threshold=1.0)\n",
    "    scheduler = None\n",
    "    \n",
    "    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8) if training_args.fp16 else None\n",
    "    print(\"padding to multiple of 8 for fp16\" if data_collator else \"not fp16\")\n",
    "    \n",
    "#     transformers.logging.set_verbosity_info()\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=ds_train_shuffled,\n",
    "        eval_dataset=ds_val_shuffled,\n",
    "        optimizers=(optimizer, scheduler)\n",
    "    )\n",
    "    trainer.add_callback(transformers.EarlyStoppingCallback(5, ))\n",
    "    \n",
    "    trainer.args._n_gpu = 2\n",
    "    trainer.train()\n",
    "    trainer.save_model(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper for freeing GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR =  f'{OUTPUT_DIR}/model'\n",
    "MODEL_DIR =  f'{OUTPUT_DIR}/checkpoint-10000'\n",
    "model_ft = T5ForConditionalGeneration.from_pretrained(MODEL_DIR)\n",
    "model_ft = model_ft.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_val.set_format(type='torch', columns=COLS_TO_FORMAT, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_from_sentence(model_ft, tokenizer, \"#1: I went for a steak dinner. I invited my roommate.\"))\n",
    "print(generate_from_sentence(model_ft, tokenizer, \"#6: I went for a steak dinner. I invited my roommate.\"))\n",
    "preds = generate_from_dataset(model_ft, tokenizer, ds_val, batch_size=128)\n",
    "preds_decoded = decode_seqs(preds, tokenizer, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_num == '2b':\n",
    "    sources_decoded = decode_seqs(ds_val['input_ids'], False)\n",
    "    sources_decoded = [x.split('</s>', 1)[0] for x in sources_decoded]\n",
    "else:\n",
    "    sources_decoded = decode_seqs(ds_val['input_ids'], True)\n",
    "labels_decoded = decode_seqs(ds_val['labels'], True)\n",
    "\n",
    "output = pd.DataFrame({'input': sources_decoded, 'output_true': labels_decoded, 'output_pred': preds_decoded, 'target': df_val['target']})\n",
    "output.to_csv(OUTPUT_DIR + \"/predictions_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TO_PRINT = 5\n",
    "for i in range(NUM_TO_PRINT):\n",
    "    ex = output.iloc[i*100]\n",
    "    print(f'EX {i*100}')\n",
    "    print('INPUT:  ', ex['input'], '\\n')\n",
    "    print('GOLD:   ', ex['output_true'], '\\n')\n",
    "    print('PRED:   ', ex['output_pred'], '\\n')\n",
    "    print('TARGET: ', ex['target'])\n",
    "    print('-' * 50, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:stories] *",
   "language": "python",
   "name": "conda-env-stories-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
